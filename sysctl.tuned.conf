# Kernel sysctl configuration file for 2.6+ kernels
#
# For binary values, 0 is disabled, 1 is enabled.  See sysctl(8) and sysctl.conf(5) for more details.

# KWA Note: this sysctl.conf is mainly targeted at web or application servers using 1Gbps connexions, and expecting a high number of concurrent connections.
# Keys and values that should be tuned are not commented, and the default value should be appended in a comment.
# Keys and values that sould not be tuned (except in very particuliar scenarii) are commented.

# KWA Note: merged from :
# - Some reference files
# - http://fasterdata.es.net/host-tuning/linux/
# - http://fasterdata.es.net/host-tuning/linux/expert/
# - http://xgu.ru/wiki/TCP_tuning
# TODO: Should merge with http://www.cyberciti.biz/faq/linux-kernel-etcsysctl-conf-security-hardening/

# Also see :
#http://serverfault.com/questions/357799/improving-tcp-performance-over-a-gigabit-network-with-lots-of-connections-and-hi

# Reference on sysctl tcp options :
#http://www.frozentux.net/ipsysctl-tutorial/ipsysctl-tutorial.html#TCPVARIABLES

# Apply with :
#  sysctl -p <path/to/file>
#  service network restart
# Return to default :
#  sysctl -p
#  service network restart



######### SYSTEM TUNING #########

# Controls the System Request debugging functionality of the kernel
kernel.sysrq = 0
# Controls whether core dumps will append the PID to the core filename
# Useful for debugging multi-threaded applications
kernel.core_uses_pid = 1
# Controls the maximum size of a message, in bytes
kernel.msgmnb = 65536
# Controls the default maxmimum size of a mesage queue
kernel.msgmax = 65536
# Controls the maximum shared segment size, in bytes
kernel.shmmax = 68719476736
# Controls the maximum number of shared memory segments, in pages
kernel.shmall = 4294967296

# Increase system-wide file descriptor limit.
# Note: this will notably constraint the maximum number of opened sockets ; you need to increase this if you want to support hundreds of thousands of concurrent connections (C100k and more). In that case, ensure that the tcp_rmem & tcp_wmem are tuned accordingly
# Note bis: don't forget to configure the ulimit options (and/or /etc/security/limits.conf file) accordingly (with syntax "<user> [soft|hard] nofile <number>" : 
#  *     soft    nofile          40000
#  *     hard    nofile          40000
# Caution, as "*" means every user, but not root.
fs.file-max = 16384

## HugePages
# HugePages: Page size is set 2MB instead of 4KB, and memory used by HugePages is locked and cannot be paged out.
# So useful for processes with large heaps (and so servers with a lot of RAM)
# E.g.: To allow 12GB of HugePages memory : set the parameter to 6144 (6144*2M=12GB)
#vm.nr_hugepages=6144
# Note: change /etc/securities/limits.conf to increase soft & hard memlock values for the processes that needs a lot of RAM
# To verify it: 'cat /proc/meminfo |grep HugePages' with HugePages_Total, HugePages_Free, Hugepagesize



######### NETWORK STACK TUNING ############


##### IP tuning

### Routing activation / option
# Controls IP packet forwarding
net.ipv4.ip_forward = 0
# Controls source route verification
net.ipv4.conf.default.rp_filter = 1
# Do not accept source routing
net.ipv4.conf.default.accept_source_route = 0

# Increase total number of local ports ; useful when managing a lot of simultaneous clients
# If you have more than 128 megabytes of physical memory, the lower bound will be 32768 and the upper bound will be 61000.
net.ipv4.ip_local_port_range = 1024 65535

# Increase the length of the processor input queue
# This queue will build up in size when an interface receives packets faster than the kernel can process them. If this queue is too small (default is 300), we will begin to loose packets at the receiver, rather than on the network. 
# recommended to increase this for 1000 BT or higher 2500, for 10 GigE 30000   
net.core.netdev_max_backlog = 30000  # Default: 300

# Allows us to set if local processes should be able to bind to non-local IP addresses.
# Useful especially for load-balancing or failover scenarii.
#ip_nonlocal_bind = 1  # Default: 0


##### TCP tuning

# Socket Buffer Sizes ; following considered to be good for 1GBps network adapters
net.core.rmem_default = 1048576
net.core.wmem_default = 1048576
net.core.rmem_max = 8388608
net.core.wmem_max = 8388608
# Increase Linux autotuning TCP buffer limit (values are "min default max") 
# Following considered to be good for 1GBps network adapters
# The more latency & bandwidth, the more the maximum size KWA:verify
# To support a lot of concurrent connections, you may want to decrease the default size (ex for c500k: 4096 4096 16777216) ; be sure to have plenty of RAM, though.
net.ipv4.tcp_rmem = 4096 87380 8388608
net.ipv4.tcp_wmem = 4096 65536 8388608
# For following: default are fine KWA:verify
# Some people recommend increasing net.tcp_mem. This is not usually needed. tcp_mem values are measured in memory pages, not bytes. The size of each memory page differs depending on hardware and configuration options in the kernel, but on standard i386 computers, this is 4 kilobyte or 4096 bytes. So the defaults values are fine for most cases.
# Doing as the following disables the linux tcp buffer size autotuning
#net.ipv4.tcp_mem = 8388608 8388608 8388608

# Recommended default congestion control is htcp for high-speed networks and long links, but reno should be fine in everyday's work
# HTCP needs built-in kernel support, or '/sbin/modprobe tcp_htcp'
# To list the available ones : sysctl net.ipv4.tcp_available_congestion_control
#net.ipv4.tcp_congestion_control=htcp  # Default: bic/cubic or reno, depending on the linux version

# Recommended for hosts with jumbo frames enabled
#net.ipv4.tcp_mtu_probing=1


### Quicker socket liberation in case of failure

## Global options
# How many times it should retry to get to a host before reaching a decision that something is wrong and that it should report the suspected problem to the network layer. 
# Should be between 3 and 100, but default is fine (if too big: timeouts will be worse than horrible)
#net.ipv4.tcp_retries1 = 3	
# How many times to retry before killing an alive TCP connection
net.ipv4.tcp_retries2 = 5  # Default: 15

## Upon starting a new connection
# How many times to try to retransmit the initial SYN packet for an active TCP connection attempt.
# The default setting is 5, which would lead to an aproximate of 180 seconds delay before the connection times out.
#net.ipv4.tcp_syn_retries = 1   # Default: 5
# How many times to retransmit the SYN,ACK reply to an SYN request. 					
#net.ipv4.tcp_synack_retries = 5

## Connection drop detection and connection reuse tuning
# TCP keepalive tuning
# How long to wait for a reply on each keepalive probe. 75 should be the max. (i.e. tuning it is reducing it)
net.ipv4.tcp_keepalive_intvl = 15  # Default: 75
# How many TCP keepalive probes to send out before it decides a specific connection is broken.
net.ipv4.tcp_keepalive_probes = 5  # Default: 9
# How often to send TCP keepalive packets to keep an connection alive if it is currently unused.
# If reduced too much, can cause too much overhead
net.ipv4.tcp_keepalive_time	= 1800  # Default: 7200	

## Upon closing a connection
# Decrease the time default value for tcp_fin_timeout connection
# The tcp_fin_timeout variable tells kernel how long to keep sockets in the state FIN-WAIT-2 if you were the one closing the socket. 
net.ipv4.tcp_fin_timeout = 30 # Default: 60
# How many TCP sockets that are not attached to any user file handle to maintain.
# Generally you should not rely on this limit, nor should you lower it artificially. If you hit this limit, you may also tune your network services a little bit to linger and kill sockets in this state more aggressively.
#net.ipv4.tcp_max_orphans = 8192  # Default: 8192
# How many times to retry to kill connections on the other side before killing it on our own side.
# If your machine runs as a highly loaded http server it may be worth thinking about lowering this value. 
net.ipv4.tcp_orphan_retries = 7  # Default: 7


### Others

# Controls the use of TCP syncookies
# The tcp_syncookies variable is used to send out so called syncookies to hosts when the kernels syn backlog queue for a specific socket is overflowed. This means that if our host is flooded with several SYN packets from different hosts, the syn backlog queue may overflow, and hence this function starts sending out cookies to see if the SYN packets are really legit.
# This variable is used to prevent an extremely common attack that is called a "syn flood attack".
net.ipv4.tcp_syncookies = 1  # Default: 0

# This variable implements a bug in the TCP protocol so it will be able to talk to certain other buggy TCP stacks.
# Implementing this bug workaround will not break compatibility from our host to others, but it will make it possible to speak to those bad stacks.
#net.ipv4.tcp_retrans_collapse = 1  #Default: 1

# Hum... Read the rfc for this. KWA: I'm not really sure about what to write about this one...
# net.ipv4.tcp_rfc1337 = 1

# Turns on Explicit Congestion Notification in TCP connections. 
# KWA: I'm not really sure about what to write about this one... I'd say maybe more relevant in a WAN (i.e. with several paths to the same destination)
# net.ipv4.tcp_ecn = 0  # Default: 0

# Disable SACK
# Note: Activating SACK is especially good on very lossy connections (connections that loose a lot of data in the transfer) since this makes it possible to only retransmit specific parts of the TCP window which lost data and not the whole TCP window.
# Some experts also say to set net.ipv4.tcp_timestamps and net.ipv4.tcp_sack to 0, as doing that reduces CPU load. We disagree with that recommendation, as we have observed that the default value of 1 helps in more cases than it hurts. But if you are extremely CPU bound you might want to experiment with turning those off.
# A good explanation : http://serverfault.com/questions/10955/when-to-turn-tcp-sack-off
# KWA Note: there seems to be a hard time deciding on this one ; is this really a bad id on GBps networks ?
net.ipv4.tcp_sack = 0  # Default: 1


### Following are apparently generally bad ideas
# Turn off the tcp_timestamps
# Some people recommend disabling tcp_timestamps. We do not recommend this for high-speed networks. It may help for home users on slow networks, as timestamps add an additional 10 bytes to each packet. But more accurate timestamp make TCP congestion control algorithms work better, and are recommended for fast networks.
# Only exception : turn off if on an extremely slow connection such as a 56 kbps modem connection to the Internet.
#net.ipv4.tcp_timestamps = 0
# Turn off the tcp_window_scaling
#net.ipv4.tcp_window_scaling = 0
# Disable tcp overflow
# Reset new connections if the system is currently overflowed with new connection attempts that the daemon(s) can not handle.
# Avoid enabling this option except as a last resort since it most definitely harm your clients. Before considering using this variable you should try to tune up your daemons to accept connections faster.
#net.ipv4.tcp_abort_on_overflow = 0
# How many bytes to reserve for a specific TCP window in the TCP sockets memory buffer where the specific TCP window is transfered in.
# Do not change this value unless you know what you are doing. Default value for this variable is 31 and should in general be a good value. 
#net.ipv4.tcp_app_win = 31  # Default: 31


##### UDP tuning
# Tuning UDP/IP
# Note: TCP window size actually effects UDP as well on Linux.
#net.ipv4.udp_wmem_min = 65536
#net.ipv4.udp_rmem_min = 65536
#net.ipv4.udp_mem = 144864  193152  289728



# Flush routes now to restart stats from scratch 
net.ipv4.route.flush = 1
